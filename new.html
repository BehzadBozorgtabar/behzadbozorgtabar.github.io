
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Visual Instruction Tuning">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLaVA</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation</h1>
            <h3 class="title is-3 publication-title">ICLR 2024</h3>
            <!--<h5 class="subtitle is-5 publication-awards">NeurIPS 2023 (Oral)</h5>-->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://people.epfl.ch/devavrat.tomar?lang=en" style="color:#f68946;font-weight:normal;">Devavrat Tomar<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://people.epfl.ch/guillaume.vray?lang=en&cvlang=en" style="color:#f68946;font-weight:normal;">Guillaume Vray<sup>*</sup></a>,
              </span>
              <span class="author-block">
    <a href="https://people.epfl.ch/jean-philippe.thiran" style="font-weight:normal;">
      <span style="color:#f68946;">Jean-Philippe</span> <span style="color:#008AD7;">Thiran</span>
    </a>,
  </span>

              <span class="author-block">
  <a href="https://behzadbozorgtabar.com/" style="font-weight:normal;">
    <span style="color:#f68946;">Behzad</span> <span style="color:#008AD7;">Bozorgtabar</span>
  </a>
</span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> EPFL</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> CHUV</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=xyxU99Nutg" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/devavratTomar/unmixtns" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>




                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          ðŸ”¥<span style="color: #ff3860">[NEW!]</span> LLaVA-1.5 achieves SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods that use billion-scale data.
          <br><br>
          LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna
          for general-purpose visual and language understanding,
          achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.
        </h4>
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Summary</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We introduce a novel test-time normalization scheme <strong>(UnMix-TNS)</strong> designed to withstand the
challenges posed by label temporal correlation of test data streams.</li>
            <li>
              <strong>UnMix-TNS</strong> demonstrates robustness across various test-time distribution shifts such as <strong>single domain</strong>, and <strong>continual domain</strong>. While not primarily designed for <strong>mixed domain</strong> settings, it offers a level of adaptability in these scenarios. Additionally, the method excels with small batch sizes, even down to individual samples.
            </li>
            <li>
              <strong>UnMix-TNS</strong> layers, with <strong>negligible inference latency</strong>, seamlessly integrate into pre-trained neural
network architectures, fortified with BN layers, augmenting test-time adaptation strategies while
incurring minimal overhead.
            </li>
            <li>
              We present empirical evidence showcasing significant performance improvements of state-of-the-art TTA methods in non-i.i.d. settings when augmented with <strong>UnMix-TNS</strong>, as benchmarked
on datasets involving common corruptions shifts and natural shifts.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>




  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Motivation</h2>
          <div class="content has-text-justified">
            <p>
              To elucidate the motivation of our method, let's compare it with the conventional approach of test-time batch normalization (TBN), using a toy example for clarity.
              <ol type="1">
                <li><b>I.I.D Setting</b>. <span style="font-size: 95%;">Under i.i.d setting, the feature statistics estimated from the current batch (pink dots) is close to the true statistics of the test distribution. </span></li>
                <li><b>Non-I.I.D Setting</b>. <span style="font-size: 95%;">Under non-i.i.d. setting (i.e. when the test features are label temporally correlated), the feature statistics estimated from the current batch are biased towards one category.</li>
                <li><b>Our Method.</b> To address this imbalance issue, we introduce a novel strategy termed <it><b>UnMix-TNS</b> (<b>Un-Mix</b>ing <b>T</b>est-Time <b>N</b>ormalization <b>S</b>tatistics)</it> simulating the i.i.d. setting by splitting the statistics of the test distribution into K components (4 in the above example), and estimating them using only the test features closest to the individual component.</li>


                  <div style="display: flex; justify-content: space-around; align-items: center;">

  <!-- First subfigure -->
  <img id="teaser1" src="images/ref.png" alt="unmix-tns" style="max-width: 430px; margin-top: -20px;">

  <!-- Second subfigure -->
  <img id="teaser2" src="images/tbn_iid.gif" alt="another-image" style="max-width: 300px;">

  <!-- Add more subfigures as needed -->
  <img id="teaser3" src="images/tbn_non_iid.gif" alt="yet-another-image" style="max-width: 300px;">
  <!-- Add more subfigures as needed -->
  <img id="teaser4" src="images/unmix_tns_non_iid.gif" alt="yet-last-image" style="max-width: 300px;">

</div>
          <figcaption style="text-align: center;">
            <strong>Figure:</strong> Illustration comparing UnMix-TNS with the TBN approach using a toy example in i.i.d. and non-i.i.d. settings.
          </figcaption>
              </ol>
           </p>

          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="column is-full-width">
    <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <h2 class="title is-2">Method: UnMix-TNS</h2>
      <div class="content">
          <h2 class="title is-4"> 1. Distribution of \(K\) UnMix-TNS Components Mixture</h2>
          <div class="image-container">
          <figure>
            <img id="taskaffinity" src="images/unmixtns_method.png" alt="Task Affinity Diagram">
            <figcaption style="text-align: center;">
              <strong>Figure:</strong> <strong>An Overview of UnMix-TNS.</strong> Given a batch of non-i.i.d test features \( \mathbf{z^t} \in \mathbb{R}^{B \times C \times L} \) at a temporal instance \( t \), we mix the instance-wise statistics \( (\tilde{\mu}^t, \tilde{\sigma}^t) \in \mathbb{R}^{B \times C} \) with \( K \) UnMix-TNS components. The alignment of each sample in the batch with the UnMix-TNS components is quantified through similarity-derived assignment probabilities \( p_k^t \). This aids both the <em>mixing</em> process and subsequent component updates for time \( t+1 \).
          </figure>
        </div>
          <p>Let \( [\mu_1^t, \dots, \mu_K^t] \) and \( [\sigma_1^t, \dots, \sigma_K^t] \) denote the statistics of the \( K \) components within the UnMix-TNS layer at a given temporal instance \( t \), where each \( \mu_k^t, \sigma_k^t \in \mathbb{R}^C \). We articulate the distribution \( h_Z^t(z) \) of instance-wise test features \( z \in \mathbb{R}^C \) marginalized over all labels at a temporal instance \( t \) using the \( K \) components:</p>

<p>\[ h_{Z_c}^t(z_c) = \frac{1}{K}\sum_k \mathcal{N}(\mu_{k,c}^t, \sigma_{k,c}^t), \]</p>

<p>where \( \mathcal{N} \) represents the Normal distribution function. Given only \( (\mu_k^t, \sigma_k^t)_{k=1}^{K} \), one can derive the label unbiased normalization test statistics \( (\bar{\mu}^t, \bar{\sigma}^t) \) at time \( t \) as follows:</p>

<p>\[
\bar{\mu}^t = \mathbb{E}_{h_{Z_c}^t}[z_c]=\frac{1}{K}\sum_k \mu_{k,c}^t,
\]</p>

<p>\[
(\bar{\sigma}^t)^2 = \mathbb{E}_{h_{Z_c}^t}[(z_c-\bar{\mu}^t)^2]=\frac{1}{K}\sum_k (\sigma_{k,c}^t)^2 + \frac{1}{K}\sum_k (\mu_{k,c}^t)^2 - \Big(\frac{1}{K}\sum_k\mu_{k,c}^t\Big)^2,
\]</p>

<p>Subsequent sections will delve into the initialization scheme for the \( K \) UnMix-TNS components and elucidate the process of updating their individual statistics \( (\mu_k^t, \sigma_k^t) \) at temporal instance \( t \).</p>

        </div>
      </div>
      <br>
        <!-- Task Affinity -->
        <div class="content">
          <h2 class="title is-4">2. Initializing UnMix-TNS Components</h2>
          <div class="flex-container">

            <!-- Text Container -->
            <div class="text-container">
              <p>Consider the pair \( (\mu,\sigma) \in \mathbb{R}^C \), which stands as the stored means and standard deviation of the features in a given BN layer of \( f_\theta \), derived from the source training dataset before adaptation.</p>
              <p>We initialize statistics \( (\mu_k^t, \sigma_k^t) \) of \( K \) components of UnMix-TNS at \( t=0 \), as delineated in the following Equation:</p>
              <p>\[
              \mu_{k,c}^0 = \mu_c + \sigma_c \sqrt{\frac{\alpha \cdot K}{K-1}} \cdot \zeta_{k,c}, \quad \sigma_{k,c}^0 = \sqrt{1-\alpha}\cdot\sigma_c, \quad \zeta_{k,c} \sim \mathcal{N}(0, 1)
              \]</p>
              <p>where \( \zeta_{k,c} \) is sampled from the standard normal distribution \( \mathcal{N}(0, 1) \), and \( \alpha \in (0, 1) \) is a hyperparameter that controls the extent to which the means of the UnMix-TNS components deviate from the stored means of BN layer during initialization. Note that the initial normalization statistics \( (\bar{\mu}^0, \bar{\sigma}^0) \) estimated from \( (\mu_k^0, \sigma_k^0) \) have an expected value equal to the stored statistics of the BN layer, i.e., \( \mathbb{E}[\bar{\mu}^0_k]=\mu \) and \( \mathbb{E}[(\bar{\sigma}^0_k)^2] = \sigma^2 \) for all values of \( \alpha \). </p>

            </div>
          </div>
        </div>
        <br>

        <div class="content">
          <h2 class="title is-4">3. Mask Creation of Denoising Task Routing (DTR)</h2>
          <h3 class="title is-5">A. Design principles</h2>
              <p>
                DTR utilizes the specific characteristics of denoising tasks for task routing.
                From recent findings, design of masks incorporates <strong>(1) Task Affinity</strong> and <strong>(2) Task Weights.</strong>
              </p>
              <ul>
                <li>
                  <strong>Task Affinity</strong>:
                  Denoising tasks at adjacent timesteps have a higher task affinity than those distant timesteps.
                  To incorporate this, DTR enforces denoising tasks at adjacent timesteps to have more shared channels than those distant timesteps.
                </li>
                <li>
                  <strong>Task Weight</strong>:
                  Previous loss weighting methods have shown that assinging higher weights to denoising tasks at higher timesteps can improve diffusion models.
                  DTR incorporates this by assigning more channels to denoising tasks at higher timesteps.
                </li>
            </ul>
          <h3 class="title is-5">B. Routing Mask</h2>

            <div class="flex-container2">
              <!-- Image Container -->
              <div class="image-container2">
              <figure>
                <img id="taskaffinity" src="images/.png" alt="Task Affinity Diagram">
                <figcaption style="text-align: center;">
                  <strong>Figure:</strong> Routing mask according to various \(\alpha \).
              </figure>
            </div>
            <!-- Text Container -->
            <div class="text-container2">
              <p>
                The mask \(m_{D_t}\) is created as:
                <p>
                  \[ m_{D_t, c} = \begin{cases}
                  1, & \text{if } \lfloor(C - C_\beta) \cdot \left(\frac{t-1}{T}\right)^{\alpha}\rceil < c \leq \lfloor(C - C_\beta) \cdot \left(\frac{t}{T}\right)^{\alpha}\rceil + C_\beta,\\
                  0, & \text{otherwise.}
                  \end{cases} \]
                </p>
                <ul>
                  <li>
                    \(C_\beta\): activated number of channels for each task, which is represented as \(\beta \cdot C\). \((0<\beta< 1) \).
                  </li>
                  <li>
                    <strong>Task Affinity</strong>: In above equation, activated channels are shifted as sliding window, enforcing denoising tasks at adjacent timesteps to have more shared channels.
                  </li>
                  <li>
                    <strong>Task Weight</strong>: By introducing hyperparameter \(\alpha > 1\), shifting of sliding window less occurs for lower timesteps,
                    allwoing  more task-dedicated channels to higher timesteps.
                  </li>
              </ul>
              </p>
            </div>
          </div>
        </div>
    </div>
  </section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"> Multimodal Instrucion-Following Data</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p>
          Based on the COCO dataset, we interact with language-only GPT-4, and collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. Please check out ``LLaVA-Instruct-150K''' on
          <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K">[HuggingFace Dataset]</a>.

<!-- CSS Code: Place this code in the document's head (between the 'head' tags) -->
<style>
  table.GeneratedTable {
    width: 100%;
    background-color: #ffffff;
    border-collapse: collapse;
    border-width: 2px;
    border-color: #c1c4c5;
    border-style: solid;
    color: #000000;
  }

  table.GeneratedTable td, table.GeneratedTable th {
    border-width: 2px;
    border-color: #9b9d9e;
    border-style: solid;
    padding: 3px;
  }

  table.GeneratedTable thead {
    background-color: #6691ee;
  }
  </style>

  <!-- HTML Code: Place this code in the document's body (between the 'body' tags) where the table should appear -->
  <div class="column is-six-fifths" width="80%">
  <table class="GeneratedTable">
    <thead>
      <tr>
        <th>Data file name</th>
        <th>File Size</th>
        <th>Sample Size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/conversation_58k.json">conversation_58k.json</a> </td>
        <td>126 MB</td>
        <td>58K</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/detail_23k.json">detail_23k.json</a></td>
        <td>20.5 MB</td>
        <td>23K</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/complex_reasoning_77k.json">complex_reasoning_77k.json</a></td>
        <td>79.6 MB</td>
        <td>77K</td>
      </tr>
    </tbody>
  </table>
</div>
  <!-- Codes by Quackit.com -->

        </p>
        <p>
          For each subset, we visualize the root noun-verb pairs for the instruction and response. For each chart, please click the link for the interactive page to check out the noun-verb pairs whose frequency is higher the given number.
        </p>
      </div>






      <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_instruction_verb_noun_50.png">
          <figcaption>
            Instruction: Conversation [<a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_instruction_verb_noun_0.html">0</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_instruction_verb_noun_20.html">20</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_instruction_verb_noun_50.html">50</a>]
          </figcaption>
        </figure>
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/detail_23k_instruction_verb_noun_0.png">
          <figcaption>
            Instruction: Detailed Description  [<a href="images/LLaVA-Instruct-150K_noun_verb/detail_23k_instruction_verb_noun_0.html">0</a>]
          </figcaption>
        </figure>
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_instruction_verb_noun_50.png">
          <figcaption>
            Instruction: Complex Reasoning   [<a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_instruction_verb_noun_0.html">0</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_instruction_verb_noun_20.html">20</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_instruction_verb_noun_50.html">50</a>]
          </figcaption>
        </figure>
      </div>
      </div>



      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
          <figure style="text-align: center;">
            <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_response_verb_noun_50.png">
            <figcaption>
              Response: Conversation [<a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_response_verb_noun_0.html">0</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_response_verb_noun_20.html">20</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/conversation_58k_response_verb_noun_50.html">50</a>]
            </figcaption>
          </figure>
          <figure style="text-align: center;">
            <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/detail_23k_response_verb_noun_50.png">
            <figcaption>
              Response: Detailed Description  [<a href="images/LLaVA-Instruct-150K_noun_verb/detail_23k_response_verb_noun_0.html">0</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/detail_23k_response_verb_noun_20.html">20</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/detail_23k_response_verb_noun_50.html">50</a>]
            </figcaption>
          </figure>
          <figure style="text-align: center;">
            <img id="teaser" width="100%" src="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_response_verb_noun_50.png">
            <figcaption>
              Response: Complex Reasoning   [<a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_response_verb_noun_0.html">0</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_response_verb_noun_20.html">20</a>, <a href="images/LLaVA-Instruct-150K_noun_verb/complex_reasoning_77k_response_verb_noun_50.html">50</a>]
            </figcaption>
          </figure>
        </div>
        </div>

    </div>
  </div>


</section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p>
          LLaVa connects pre-trained <a href="https://openai.com/research/clip">CLIP ViT-L/14</a> visual encoder and large language model <a href="https://github.com/lm-sys/FastChat">Vicuna</a>, using a simple projection matrix.   We consider a two-stage instruction-tuning procedure:
          <ul type="1">
            <li><b>Stage 1: Pre-training for Feature Alignment</b>. <span style="font-size: 95%;">Only the projection matrix is updated, based on a subset of CC3M.</span></li>
            <li><b>Stage 2: Fine-tuning End-to-End</b>. <span style="font-size: 95%;">Both the projection matrix and LLM are updated for two different use senarios:
              <ul type="1">
                <li> <b>Visual Chat</b>: LLaVA is fine-tuned on our generated multimodal instruction-following data for daily user-oriented applications.
                <li> <b>Science QA</b>: LLaVA is fine-tuned on this multimodal reasonsing dataset for the science domain.</span></li>
              </ul>
          </ul>
          Please check out our
          <a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md">[Model Zoo]</a>.
        </p>
      </div>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="images/llava_arch.png">
        </div>


      </centering>
    </div>
  </div>


</section>



<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"> Performance</h2>
    </div>
  </div>



  <!-- </div> -->
  <!--/ Results. -->
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><img id="painting_icon" width="4%" src="https://cdn-icons-png.flaticon.com/512/1698/1698535.png"> <span style="font-size: 100%;">Visual Chat:</span> Towards building multimodal GPT-4 level chatbot  </h2>

      <div>
        <a href="https://plotly.com/~lichunyuan24/5/?share_key=d78QObaCAYCIy8PJpe3gd1" target="_blank" title="llava_gpt4_pie" style="display: block; text-align: center;">  <img id="painting_icon" width="90%" src="images/pie_llava_gpt4.png"> </a>

    </div>

    <p style="font-family:Times New Roman"><b>An evaluation dataset with 30 unseen images is constructed: each image is assocaited with three types of instructions: conversation, detailed description and complex reasoning. This leads to 90 new language-image instructions, on which we test LLaVA and GPT-4, and use GPT-4 to rate their responses from score 1 to 10. The summed score and relative score per type is reported. Overall, LLaVA achieves 85.1% relative score compared with GPT-4, indicating the effectinvess of the proposed self-instruct method in multimodal settings</b>
    </div>
  </div>

  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"> <img id="painting_icon" width="3%" src="https://scienceqa.github.io/img/logo.png"><span style="font-size: 100%;"> Science QA:</span> New SoTA with the synergy of LLaVA with GPT-4</h2>

      <div>
        <a href="https://plotly.com/~lichunyuan24/1/?share_key=v4opE3TJpxqQ08RYsDD4iv" target="_blank" title="Plot 1" style="display: block; text-align: center;"><img id="painting_icon" width="65%" src="images/bar_llava_gpt4_scienceqa.png"></a>
        <script data-plotly="lichunyuan24:1" sharekey-plotly="v4opE3TJpxqQ08RYsDD4iv" src="https://plotly.com/embed.js" async></script>
    </div>
        <p style="font-family:Times New Roman"><b>LLaVA alones achieve 90.92%. We use the text-only GPT-4 as the judge, to predict the final answer based on its own previous answers and the LLaVA answers. This "GPT-4 as judge" scheme yields a new SOTA 92.53%.</b>

    </div>
  </div>
</section>




<section class="section">

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"> Examples on Visual Instruction Following</h2>
    </div>
  </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
         <h2 class="title is-4">Visual Reasoning on two examples from <a href="https://arxiv.org/abs/2303.08774">OpenAI GPT-4 Technical Report</a></h2>
      </div>
      </div>

    <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <img id="teaser" width="35%" src="images/cmp_ironing.png">
      <img id="teaser" width="38%" src="images/cmp_chicken_nugget.png">
    </div>
    </div>



    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
         <h2 class="title is-4">Optical character recognition (OCR)</a></h2>
      </div>
      </div>

    <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <img id="teaser" width="32%" src="images/ocr/llava_example_cvpr2023.png">
        <img id="teaser" width="32%" src="images/ocr/llava_example_cvinw_logo.png">
        <img id="teaser" width="32%" src="images/ocr/example_llava_exmaple.png">
    </div>
    </div>






  <div class="container mt-5">
    <!-- <h2 class="text-center mb-5">Who's GPT-4's favorite? Battles between State-of-the-Art Chatbots</h2> -->
    <!-- Selection -->
    <div class="form-row" style="justify-content: flex-end;">
      <div class="form-group col-md-1">
        <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
        <div class="btn-group" role="group" aria-label="Left and Right Controller"
          style="width: 100%;align-items: flex-end;justify-content: center;flex-direction: row;display: flex;">
          <button type="button" class="form-control btn btn-primary" id="prev-question"><i
              class="material-icons">keyboard_arrow_left</i></button>
          <button type="button" class="form-control btn btn-primary" id="next-question"><i
              class="material-icons">keyboard_arrow_right</i></button>

        </div>
      </div>
    </div>

    <!-- Question Card -->
    <div style="display: flex; justify-content: center; align-items: center;">
      <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
        <!-- <p><b>Description:</b> Monalisa is a famous painting by Leonardo da Vinci. </p> -->

        <div class="card-body" id="selected-question" style="display: flex; height: 80vh;">
          <div class="chat-history">
            <!-- Add your chat messages here -->
          </div>

        </div>
      </div>
    </div>

  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@inproceedings{tomar2024UnMixTNS,
  title={Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation},
  author={Tomar, Devavrat and Vray, Guillaume and Thiran, Jean-Philippe and Bozorgtabar, Behzad},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
    }
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://llava-vl.github.io/">LLaVA</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
</section>

</body>

</html>
