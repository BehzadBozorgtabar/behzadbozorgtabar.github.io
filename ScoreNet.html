<!--This HTML is based on Taesung Park's project page. Code from w3schools was also used.-->
<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .res_wrapper{
        text-align: center;
        float: left;
        margin-left: 3px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }

    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .collapsible {
        background-color: #777;
        color: white;
        cursor: pointer;
        padding: 18px;
        width: 100%;
        border: none;
        text-align: left;
        outline: none;
        font-size: 20px;
    }

    .active, .collapsible:hover {
        background-color: #555;
    }

    .collapsible:after {
        content: '\002B';
        color: white;
        font-weight: bold;
        float: right;
        margin-left: 5px;
    }

    .active:after {
        content: "\2212";
    }

    .content {
        display: none;
        overflow: hidden;
        background-color: #f1f1f1;
    }

</style>

<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-176302866-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-176302866-1');
    </script>

    <title>ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification</title>
    <meta property="og:image" content="" />
    <meta property="og:title" content="ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. In WACV, 2023." />
</head>

<body>
<br>
<center>
    <span style="font-size:38px">ScoreNet:<br> Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification</span><br>
    <table align=center width=1100px>
        <tr>
            <td align=center width=275px>
                <span style="font-size:24px"><a href="https://people.epfl.ch/thomas.stegmuller?lang=en">Thomas Stegmüller</a></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <span style="font-size:24px"><a href="https://behzadbozorgtabar.com/">Behzad Bozorgtabar</a></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <span style="font-size:24px"><a href="https://ch.linkedin.com/in/antoine-spahr-370222187">Antoine Spahr</a></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <span style="font-size:24px"><a href="https://people.epfl.ch/jean-philippe.thiran">Jean-Philippe Thiran</a></span>
            </td>
        </tr>
    </table>
    <table align=center width=700px>
        <tr>
            <td align=center width=10px>
                <center>
                    <span style="font-size:20px"></span>
                </center>
            </td>
            <td align=center width=225px>
                <center>
                    <span style="font-size:24px">EPFL</span>
                </center>
            </td>
            <td align=center width=225px>
                <center>
                    <span style="font-size:24px">CHUV</span>
                </center>
            </td>
            <td align=center width=10px>
                <center>
                    <span style="font-size:24px"></span>
                </center>
            </td>
        </tr>
    </table>

    <span style="font-size:24px"> <a href="https://arxiv.org/pdf/2202.07570.pdf">paper</a>   </span><br>
    <span style="font-size:24px"> WACV 2023 </span>


</center>

<br>

<table align=center width=900px>
    <tr>
        <td align=center width=900px>
            <center>
        <td><img class="round" style="width:900px" src="images/scorenet_2.png"/></td>
        </center>
        </td>
    </tr>
    <hr>
<!--<div class="column-right pxb">
<table align="center" width="700px">
    <tbody>
    <tr><center><a href="http://efrosgans.eecs.berkeley.edu/HessianPenalty/hessian_penalty_90sec_talk.mov">Download video</a></center></tr>
    <tr>
        <td align="center" width="700px">
            <center>
                <iframe width="700" height="394" src="https://www.youtube.com/embed/jPl-0EN6S1w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </center>
        </td>
    </tr></tbody></table>
<hr>
</div>-->
<table align=center width=900px>
    <center><h1>Abstract</h1></center>
    <tr>
      Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning (MIL) to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose <i>ScoreNet</i>, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely <i>ScoreMix</i>, by leveraging the image’s semantic distribution to guide the data mixing and produce coherent sample-label pairs. <i>ScoreMix</i> is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest (TRoIs) classification. <i>ScoreNet</i> equipped with proposed <i>ScoreMix</i> augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, <i>ScoreNet</i> yields high efficacy and outperforms SOTA efficient transformers, namely TransPath and SwinTransformer, with throughput around 3× and 4× higher than the aforementioned architectures, respectively.
        <br>
    </tr>
    <br>
    <hr>
<!--<div class="column-right pxb">
    <center><h1>Video (10 min)</h1></center>
    <table align="center" width="700px">
        <tbody>
        <tr><center><a href="http://efrosgans.eecs.berkeley.edu/HessianPenalty/hessian_penalty_spotlight_talk.mp4">Download video</a></center></tr>
        <tr>
            <td align="center" width="700px">
                <center>
                    <iframe width="700" height="394" src="https://www.youtube.com/embed/uZyIcTkSSXA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </center>
            </td>
        </tr></tbody></table>
    <hr>
</div>-->
<!-- <tr><center>
    <center><h1>Complete Results</h1></center>
    <p>
        Below, we show <i>all</i> z components learned by (1) ProGAN fine-tuned with the Hessian Penalty, (2) ProGAN trained from scratch with the Hessian Penalty, (3) ProGAN trained with InfoGAN loss and (4) vanilla ProGAN. Each row corresponds to varying a single z component from -2 to +2, leaving the remaining ones fixed. Different columns in the same row re-sample the fixed z components. All models were trained with 12-dimensional latent z vectors---with the exception of CLEVR-U which used |z|=3---hence why there are 12 rows. These results are <b>not</b> cherry-picked.
    </p>
    <table align=center width=800px>
        <tr>
            <td>
                <button class="collapsible">Edges+Shoes </button>
                <div class="content">
                    <h1 align=center>Edges+Shoes</h1>
                    <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/edgeshoes/e2s_ft_025656/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty<br>(Fine-Tuned)</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/edgeshoes/e2s_fs_025656/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/edgeshoes/e2s_info_025355/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>InfoGAN</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/edgeshoes/e2s_bl_024149_wandb_ver/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Vanilla</p></div>
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <button class="collapsible">CLEVR-Simple</button>
                <div class="content">
                <h1 align=center>CLEVR-Simple</h1>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_simple/simple_ft_033795/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty<br>(Fine-Tuned)</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_simple/simple_fs_033192/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_simple/simple_info_032589/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>InfoGAN</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_simple/simple_bl_031685/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Vanilla</p></div>
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <button class="collapsible">CLEVR-1FOV</button>
                <div class="content">
                <h1 align=center>CLEVR-1FOV</h1>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_1fov/1fov_ft/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty<br>(Fine-Tuned)</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_1fov/1fov_fs/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_1fov/1fov_info/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>InfoGAN</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_1fov/1fov_bl_BEST/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Vanilla</p></div>
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <button class="collapsible">CLEVR-U</button>
                <div class="content">
                <h1 align=center>CLEVR-U</h1>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_u/underparam_ft_034966/seed_0/z000_to_z003.mp4"  style="width:195px"></video><p>Hessian Penalty<br>(Fine-Tuned)</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_u/underparam_fs_036507/seed_0/z000_to_z003.mp4"  style="width:195px"></video><p>Hessian Penalty</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_u/underparam_info_030781/seed_0/z000_to_z003.mp4"  style="width:195px"></video><p>InfoGAN</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_u/underparam_bl/seed_0/z000_to_z003.mp4"  style="width:195px"></video><p>Vanilla</p></div>
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <button class="collapsible">CLEVR-Complex</button>
                <div class="content">
                <h1 align=center>CLEVR-Complex</h1>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_complex/two_obj_ft/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty<br>(Fine-Tuned)</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_complex/two_obj_fs_036206/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Hessian Penalty</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_complex/two_obj_info_016010/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>InfoGAN</p></div>
                <div class="res_wrapper"><video controls autoplay loop muted class="rounded" src="images/hessian_visuals/clevr_complex/two_obj_bl_013297/seed_0/z000_to_z012.mp4"  style="width:195px"></video><p>Vanilla</p></div>
                </div>
            </td>
        </tr>

    </table>

    <hr>
<!-- </center></tr> -->
<center><h1>ScoreNet Overview</h1></center>
<table align=center width=900px>
    <tr>
        <td align=center width=900px>
            <center>
        <td><img class="round" style="width:900px" src="images/ScoreNet_diagram.png"/></td>
        </center>
        </td>
    </tr>

<p>Below, we illustrate an overview of the proposed training pipeline for H&E stained histology TRoIs' representation learning.
Histopathological image classification requires capturing cellular and tissue-level microenvironments and learning their respective interactions. Motivated by the above, we propose an efficient transformer, <i>ScoreNet</i> that captures the cell-level structure and tissue-level context at the most appropriate resolutions. Provided sufficient contextual information, we postulate and empirically verify that a tissue's identification can be achieved by only attending to its sub-region in a high-resolution image. As a consequence, <i>ScoreNet</i> encompasses two stages. The former <i>(differentiable recommendation)</i> provides contextual information and selects the most informative high-resolution regions. The latter (<i>aggregation and prediction</i>) processes the recommended regions and the global information to identify the tissue and model their interactions simultaneously.</p>

<p>More precisely, the recommendation stage is implemented by a ViT and takes as input a downscaled image to produce a semantic distribution over the high-resolution patches. Then, the most discriminative high-resolution patches for the end task are differentiably extracted. These selected patches (tokens) are then fed to a second ViT implementing the <i>local fine-grained attention</i> module, which identifies the tissues represented in each patch. Subsequently, the embedded patches attend to one another via a transformer encoder (<i>global coarse grained  attention</i>). This step concurrently refines the tissues' representations and model their interactions. As a final step, the concatenation of the [CLS] tokens from the recommendation's stage and that of the <i>global coarse grained  attention</i>'s encoder produces the image's representation. Not only does <i>ScoreNet</i>'s workflow allows for a significantly increased throughput compared to SOTA methods, it further enables the independent pre-training and validation of its constituent parts.</p>




<hr>
    <center><h1>ScoreNet Results</h1></center>

    <p>Below, we compare the TRoIs classification performance of <i>ScoreNet</i> on the BRACS dataset against the state-of-the-arts, including MIL-based e.g., TransMIL and CLAM, GNN-based, e.g., HACT-Net, and self-supervised transformer-based approaches, e.g., TransPath. <i>ScoreNet</i> reaches a new state-of-the-art weighted F1-score of 64.4% on the BRACS TRoIs classification task outperforming the second-best method, HACT-Net, by a margin of 2.9%.
      <i>ScoreNet</i> allows for an easily tuning to meet prior inductive biases on the ideal scale for a given task.</p>

    <table align=center width=900px>
        <tr>
            <td align=center width=900px>
                <center>
            <td><img class="round" style="width:900px" src="images/ScoreNet_table.png"/></td>
            </center>
            </td>
        </tr>


    </table>
<!--
    <hr>

    <center><h1>Try our code</h1></center>

    <p>We released PyTorch code and pre-trained models of the OptTTA for your use. </p>
<!-- <tr><center>
    <table align=center width=900px>
        <tr>
            <td align=center width=900px>
                <center>
            <td><img class="round" style="width:900px" src="images/hessian_code.png"/></td>
            </center>
            </td>
        </tr>
    </table>
<!-- </center></tr> -->
<hr>

<center><h1>Try our code</h1></center>

<p>We released PyTorch code and models of the ScoreNet for your use. </p>
<!-- <tr><center>
<table align=center width=900px>
    <tr>
        <td align=center width=900px>
            <center>
        <td><img class="round" style="width:900px" src="images/hessian_code.png"/></td>
        </center>
        </td>
    </tr>
</table>
<!-- </center></tr> -->
<table align=center width=800px>
    <tr><center>
    <span style="font-size:28px"> <a href='https://github.com/stegmuel/ScoreNet'> [GitHub] </a>
                <br></span>
    </center></tr>
    <!-- <tr><center> -->
    <!-- <span style="font-size:28px"><a>&nbsp;Model: [Prototxt] Weights: [Unrescaled] [Rescaled]</a></span> -->
    <!-- </center></tr> -->
</table>

<br>

    <table align=center width=475 px>
        <hr>
        <center><h1>Paper</h1></center>
        <tr>
            <td><a href="https://openreview.net/pdf?id=B6HdQaY_iR"><img class="layered-paper-big" style="height:175px" src="images/ScoreNet_first_page.png"/></a></td>
            <td><span style="font-size:12pt">T. Stegmüller, B. Bozorgtabar, A. Spahr, J.P. Thiran</span><br>
                <span style="font-size:12pt"><b>ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification.</b></span><br>
                <span style="font-size:12pt">In WACV, 2023.</span>
                <span style="font-size:12pt"><a href="https://arxiv.org/pdf/2202.07570.pdf">arXiv</a></span>
            </td>
            </td>
        </tr>
    </table>
    <br>
    <div class="white_section_nerf wf-section">
	 <div class="citation w-container"><h2 class="grey-heading_nerf">BibTeX</h2>
	 <p class="paragraph-3 nerf_text nerf_results_text citation">@inproceedings{Stegmüller2023scorenet,<br/>
	 &emsp;&emsp;title={ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification},
	 <br/>&emsp;&emsp;author={Stegmüller, Thomas and Bozorgtabar, Behzad and Spahr, Antoine and Thiran, Jean-Philippe},<br/>&emsp;&emsp;booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},<br/>&emsp;&emsp;year={2023}<br/>}</p></div>

	</div>
	</div>

<!--
    <table align=center width=600px>
        <tr>
            <td><span style="font-size:14pt"><center>
				  	<a href="images/OptTTA_bibitex.txt">[Bibtex]</a>
            </center></span></td>
        </tr>
    </table>
  <!-- </center></tr> -->
    <br>
    <hr>

    <table align=center width=1100px>
        <tr>
            <td width=400px>
                <left>
                    <center><h1>Acknowledgements</h1></center>
                    We thank Taesung Park for his project page template.
                </left>
            </td>
        </tr>
    </table>
</table>


<br><br>

    <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                    content.style.display = "none";
                } else {
                    content.style.display = "block";
                }
            });
        }
    </script>

</body>
</html>
