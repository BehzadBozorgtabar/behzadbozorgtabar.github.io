
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Self-Supervised Learning via Cross-Image Object-Level Bootstrapping">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CrIBo</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CrIBo <img style="width: 60px; height: 60px; vertical-align: middle;" src="images/cribo_title.png">: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping</h1>
            <h3 class="title is-3 publication-title">ICLR 2024 (Spotlight)</h3>
            <!--<h5 class="subtitle is-5 publication-awards">NeurIPS 2023 (Oral)</h5>-->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.timlebailly.com/" style="color:#008AD7;font-weight:normal;">Tim Lebailly<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://people.epfl.ch/thomas.stegmuller?lang=en" style="color:#008AD7;font-weight:normal;">Thomas StegmÃ¼ller<sup>*</sup></a>,
              </span>
              <span class="author-block">
  <a href="https://behzadbozorgtabar.com/" style="color:#008AD7;font-weight:normal;">Behzad Bozorgtabar</a>
</span>
              <span class="author-block">
    <a href="https://people.epfl.ch/jean-philippe.thiran" style="color:#008AD7;font-weight:normal;">Jean-Philippe Thiran</a>,
  </span>
  <span class="author-block">
<a href="https://www.esat.kuleuven.be/psi/TT/" style="color:#008AD7;font-weight:normal;">Tinne Tuytelaars</a>
</span>



            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#7ED321; font-weight:normal">&#x25B6 </b> KU Leuven</b></span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> EPFL</b></span>
              <span class="author-block"><b style="color:#50E3C2; font-weight:normal">&#x25B6 </b> CHUV</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.07855.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=3M0GXoUEzP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>OpenReview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/tileb1/CrIBo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>




                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <strong>CrIBo</strong> is the first online SSL method that explicitly enforces cross-image consistency at the object/object-part level, showcasing excellent performance on in-context scene understanding across several dense downstream tasks.
        </h4>
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Summary</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We introduce a novel self-supervised learning (SSL) approach <strong>(CrIBo)</strong> tailored to enhance dense visual representation learning. To the best of our knowledge, <strong>CrIBo</strong> is the first end-to-end online SSL approach that explicitly enforces cross-image consistency at the object/object-part level.</li>
            <li>
              <strong>CrIBo</strong> emerges as a notably strong and adequate candidate for <em>in-context scene understanding</em> tasks without compromising the performance on standard segmentation benchmarks.
            </li>
            <li>
              <strong>CrIBo</strong> shows state-of-the-art performance on dense nearest neighbor retrieval task, compatible with scene-centric datasets, addressing a gap in the existing cross-image SSL literature.
            </li>
          </ul>

        </div>
      </div>
    </div>
  </div>
</section>




  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <div class="content has-text-justified">

                  <div style="display: flex; justify-content: space-around; align-items: center; margin-top: -50px;">

  <!-- First subfigure -->
  <img src="images/attrappe_oeil3_compressed.png" alt="Positioning of CriBO in the landscape of self-supervised learning" style="width: 100%;">


</div>
          <figcaption style="text-align: center;">
            <strong>Figure:</strong> <strong>Positioning of CriBO in the landscape of self-supervised learning.</strong> <strong>a)</strong> Illustration of the cross-image self-supervision concept. <strong>c)</strong> Depiction of object-level self-supervision. <strong>b)</strong> CrIBo benefits from both learning paradigms.
          </figcaption>
              </ol>
           </p>

          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="column is-full-width">
    <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <h2 class="title is-2">Method: CrIBo</h2>
      Our method, <strong>CrIBo</strong>, is realized through the following steps:
<ol>
    <li>Identify semantically coherent regions within the image to create object-level representations.</li>
    <li>Match pairs of object-level representations across images.</li>
    <li>Apply cross-image consistency between pairs of matching object-level representations.</li>
</ol>
<div class="image-container">
<figure>
  <img id="taskaffinity" src="images/cribo_m.png" alt="Task Affinity Diagram">
  <figcaption style="text-align: center;">
    <strong>Figure:</strong> <strong>High-level overview of cross-image object-level bootstrapping (CrIBo).</strong> Given an encoder <em>f</em> and a pair of augmented views, described as <span style="font-style: italic;">x&#x0303;<sub>1</sub></span> and <span style="font-style: italic;">x&#x0303;<sub>2</sub></span>, object representations <em>c<sub>i</sub><sup>k</sup></em> (depicted as colored object masks) from each view <em>i</em> are computed. Using a memory bank, the nearest neighbors of each object representation <em>c<sub>1</sub><sup>k</sup></em> from the first view are retrieved. A self-supervised consistency loss (depicted as colored arrows) is then enforced between <em>c<sub>2</sub><sup>k</sup></em> and its corresponding retrieved neighbor from the other view, <em>NN(c<sub>1</sub><sup>k</sup>)</em>.

</figure>
</div>

      <div class="content">
          <h2 class="title is-4"> 1. Semantically Coherent Image Regions</h2>
          <p>The first step towards cross-image object-level bootstrapping is to identify semantically coherent image regions within the images. To that end, we rely on an online joint-space clustering algorithm from <a href="https://behzadbozorgtabar.com/CrOC.html">[Stegmuller_2023_CVPR]</a>.</p>
          <h4>Joint-space clustering.</h4>
          <div>The clustering algorithm takes as input the dense representations <span class="math-tex" data-latex="\mathbf{z}_1 \in \mathbb{R}^{N \times d}">\( \mathbf{z}_1 \in \mathbb{R}^{N \times d} \)</span> and <span class="math-tex" data-latex="\mathbf{z}_2 \in \mathbb{R}^{N \times d}">\( \mathbf{z}_2 \in \mathbb{R}^{N \times d} \)</span> associated with the two augmented views and concatenates them along the token axis to obtain the dense representation of the joint-view, <span class="math-tex" data-latex="\mathbf{z}_{\text{cat}} \in \mathbb{R}^{2N \times d}">\( \mathbf{z}_{\text{cat}} \in \mathbb{R}^{2N \times d} \)</span>. The <span class="math-tex" data-latex="2N">\( 2N \)</span> tokens are subsequently partitioned into <span class="math-tex" data-latex="K">\( K \)</span> clusters to produce semantically coherent and spatially compact clusters. A hyperparameter <span class="math-tex" data-latex="\lambda_{\text{pos}}">\( \lambda_{\text{pos}} \)</span> modulates the importance of the spatiality against that of the semantic. The resulting assignment is encoded in a matrix <span class="math-tex" data-latex="\mathbf{Q}^{*} \in \mathbb{R}^{2N \times K}">\( \mathbf{Q}^{*} \in \mathbb{R}^{2N \times K} \)</span>, which can be split in two to obtain the view-wise assignments <span class="math-tex" data-latex="\mathbf{Q}^{*}_1, \mathbf{Q}^{*}_2 \in \mathbb{R}^{N \times K}">\( \mathbf{Q}^{*}_1, \mathbf{Q}^{*}_2 \in \mathbb{R}^{N \times K} \)</span>. Here <span class="math-tex" data-latex="\mathbf{Q}^{*}_i(n,k)">\( \mathbf{Q}^{*}_i(n,k) \)</span> denotes a soft-assignment of token <span class="math-tex" data-latex="n">\( n \)</span> to object-cluster <span class="math-tex" data-latex="k">\( k \)</span> in view <span class="math-tex" data-latex="i">\( i \)</span>. These can in turn be used to compute the <span class="math-tex" data-latex="K">\( K \)</span> object representations <span class="math-tex" data-latex="\mathbf{c}_i \in \mathbb{R}^{d}">\( \mathbf{c}_i \in \mathbb{R}^{d} \)</span> in each view <span class="math-tex" data-latex="i">\( i \)</span> as follows:</div>
          <p>
\[ \mathbf{c}_i^k = \sum_{n \in [N]} \mathbf{Q}^{*}_i(n,k) \cdot \mathbf{z}_i^n,\quad i \in \{1,2\} \]
</p>

          <div>Note that the columns of <span class="math-tex" data-latex="\mathbf{Q}^{*}_i">\( \mathbf{Q}^{*}_i \)</span> undergo <span class="math-tex" data-latex="l_1">\( l_1 \)</span> normalization, rendering the object representations as affine combinations of local representations. observed that the cross-view correspondence is provided for free as the object representation <span class="math-tex" data-latex="\mathbf{c}_1^k">\( \mathbf{c}_1^k \)</span> from the first view is matched to <span class="math-tex" data-latex="\mathbf{c}_2^k">\( \mathbf{c}_2^k \)</span> in the second one as a consequence of the joint-space clustering. In practice, it can be the case that some objects are exclusive to one of the two views, i.e., <span class="math-tex" data-latex="\sum_{n \in [N]} \mathbf{Q}^{*}_i(n,k) = 0">\( \sum_{n \in [N]} \mathbf{Q}^{*}_i(n,k) = 0 \)</span>. In that case, the object <span class="math-tex" data-latex="k">\( k \)</span> is simply discarded from the view where it is present.
        </div>
      </div>
      <br>
        <!-- Task Affinity -->
        <div class="content">
          <h2 class="title is-4">2. Cross-Image Object Matchings</h2>
          <div class="flex-container">

            <!-- Text Container -->
            <div class="text-container">
              <p>In the section on semantically coherent image regions, we detail the methodology for identifying objects within the views of a given image and establish the strategy for their alignment from one view to another. Following this step is the critical phase of aligning such objects across images. We rely on a pair of memory banks \( \mathcal{M}_1 \) and \( \mathcal{M}_2 \), populated with object representations extracted from the first and second views during previous iterations. Thus, the memory banks \( \mathcal{M}_{i} \) can be queried with an object representation \( \mathbf{c}_i^k \) to retrieve its nearest neighbor:</p>
              <p>\[
              \text{NN}(\mathbf{c}_i^k, \mathcal{M}_i) \triangleq \underset{\mathbf{c} \in \mathcal{M}_i}{\arg\max} \frac{\mathbf{c}^\top \mathbf{c}_i^k}{\| \mathbf{c} \|_2 \| \mathbf{c}_i^k \|_2}
              \]</p>

<p>Here, we consider \( \text{NN}(\mathbf{c}^k_1, \mathcal{M}_{1}) \) and \( \mathbf{c}_1^k \) as cross-image object-level matches.
<h4>Cycle-consistent matchings.</h4>
<div class="figure">
    <img src="images/cribo_cc.png" alt="Illustration of cycle-consistent matchings" style="width:100%;">
    <p class="caption"><strong>Figure: </strong>Illustration of cycle-consistent matchings. Such matchings are invariant to data augmentations and reciprocal, loosely speaking.</p>
</div>
<p>At the beginning of the pretraining phase, when the encoder is randomly initialized, relying on the nearest neighbor operator to identify cross-image object-level matches is not well grounded. Indeed, this straightforward approach assumes that the representations of similar objects are close in the embedding space, an assumption that does not hold at the initialization. Therefore, enforcing consistency between such pairs of semantically distinct objects should be avoided. To that end, we design a <em>semantic closeness</em> condition, which is only fulfilled when the matches are established based on high-level features, thus enabling the discarding of invalid positive pairs that do not share a semantic similarity.</p>

<p>At a high level, we consider a pair of objects as a valid positive pair if they are the nearest neighbors of each other. In practice, the criterion is implemented as follows. Starting from an object representation \( \mathbf{c}_1^k \) in the batch of first views, its nearest neighbor \( \texttt{nn}(\mathbf{c}_1^k, \mathcal{M}_1) \) in the queue is found using the nearest neighbor operator. To ensure that the criterion is invariant to data augmentation, we then switch to the queue of second views via the \( \texttt{jump}(\cdot) \) operator:</p>
<p>\[ \texttt{jump}(\mathbf{c}_1^k) \triangleq \mathbf{c}_2^k, \quad \quad \texttt{jump}(\mathbf{c}_2^k) \triangleq \mathbf{c}_1^k \]</p>
<p>From this step, we obtain \( \texttt{jump}(\texttt{nn}(\mathbf{c}_1^k, \mathcal{M}_1)) \) and retrieve its NN in the batch of second views \( \texttt{nn}(\texttt{jump}(\texttt{NN}(\mathbf{c}_1^k, \mathcal{M}_1)), \mathcal{B}_{2}) \). The final step consists of switching back to the batch of first views and verifying if the resulting object is the same as the one we started from. More formally:</p>
<p>\[
  \mathcal{O}(\mathbf{c}_i^k) \triangleq
    \begin{cases}
      true & \text{if} \quad \mathbf{c}_i^k = \texttt{jump}(\texttt{nn}(\texttt{jump}(\texttt{nn}(\mathbf{c}_i^k, \mathcal{M}_i)), \mathcal{B}_j))\\
      false & \text{otherwise}
    \end{cases}
    \quad i, j \in \{1, 2\}, i \neq j
\]</p>




            </div>
          </div>
        </div>
        <br>

        <div class="content">
          <h2 class="title is-4">3. Self-Supervised Training Objectives</h2>
          <p> CrIBoâ€™s final training  objective involves three distinct types of self-supervision: cross-view object-level, cross-image object-level, and global cross-image. All levels of self-supervision are based on the self-distillation mechanism, which employs a student-teacher pair of siamese networks \(f_s\) and \(f_t\), respectively. Given two augmented views \(\tilde{\boldsymbol{x}}_1\) and \(\tilde{\boldsymbol{x}}_2\), global representations and object representations can be obtained for each view and each network within the siamese pair.</p>

<h4>Cross-View Object-Level Self-Supervision</h4>
<p>The object representations from the teacher and student are fed to their respective head \(h_s\) and \(h_t\) to output probability mass functions whose sharpness can be modulated via temperature parameters (\(\tau_{s}\) and \(\tau_{t}\)). For the teacher, this translates to:</p>
<p>\[
\mathbf{p}_{i,t}^k = \texttt{softmax}\left(\text{head}_{t}(\mathbf{c}_{i,t}^k) / \tau_{t} \right),  \quad i \in \{1,2\}
\]</p>
<p>The student's projections are obtained analogously. The cross-view object-level loss is expressed as the averaged cross-entropy over all pairs of object representations and making the loss symmetric with respect to the student/teacher:</p>
<p>\[
\mathcal{L}_{cv}^{o} = \frac{1}{K}\sum_{k \in [K]} \left( H(\mathbf{p}_{1,t}^k, \mathbf{p}_{2,s}^k) +  H(\mathbf{p}_{2,t}^k, \mathbf{p}_{1,s}^k) \right)
\]</p>
<p>where \(H(\mathbf{a}, \mathbf{b}) = -\sum_{l=1}^{L} \mathbf{a}_{l} \log (\mathbf{b}_{l})\) denotes the cross-entropy.</p>



<h4>Cross-Image Object-Level Self-Supervision</h4>
<p>Once all object representations in the batch have been retrieved, the cross-image object-level loss is enforced in a manner analogous to its cross-view counterpart. The nearest neighbors are first fed to the projection head as follows:</p>
<p>\[
\tilde{\mathbf{p}}_{i,t}^{k} = \texttt{softmax}\left(h_{t}(\texttt{nn}(\mathbf{c}_{i,t}^k)) / \tau_{t} \right),  \quad i \in \{1,2\}
\]</p>
<p>Note that these projections are only defined for the teacher since the \(\texttt{nn}(\cdot)\) operator is not differentiable. The formulation of the cross-image object-level loss aligns with that in the cross-view loss up to the filtering of invalid positive pairs:</p>

<p>\[
\mathcal{L}_{ci}^{o} = \frac{1}{Z_1}\sum_{k \in [K]} \mathbb{1}_{\{\mathcal{O}(\mathbf{c}_1^k)\}} H(\tilde{p}_{1,t}^{k}, p_{2,s}) +  \frac{1}{Z_2}\sum_{k \in [K]} \mathbb{1}_{\{\mathcal{O}(\mathbf{c}_2^k)\}} H(\tilde{p}_{2,t}^{k}, p_{1,s})
\]</p>

<p>where \( \mathbb{1}_{\{\cdot\}} \) denotes the indicator function and where \( Z_1 \) and \( Z_2 \) are normalization constants, i.e., \( \sum_{k \in [K]} \mathbb{1}_{\{\mathcal{O}(\mathbf{c}_1^k)\}} \) and \( \sum_{k \in [K]} \mathbb{1}_{\{\mathcal{O}(\mathbf{c}_2^k)\}} \), respectively.</p>

  <h4>Global Cross-View Self-Supervision</h4>
  <p>Importantly, both the clustering and the bootstrapping steps are contingent on the quality of the representations. To avoid potential instabilities, we incorporate a global cross-view loss into our framework, offering meaningful self-supervision at any stage of the pretraining. This loss is analogous to the one from the cross-view object-level loss except that it is applied to the global representation (\(\mathbf{z}_{1,s}\), \(\mathbf{z}_{2,s}\), \(\mathbf{z}_{1,t}\), and \(\mathbf{z}_{2,t}\)), which are fed to a dedicated head \( \overline{h} \):</p>

  <p>The global cross-view projections are defined as follows:</p>
  <p>\[
  \overline{p}_{i,t} = \texttt{softmax} \left(\overline{h}_{t}(\mathbf{z}_{i,t}) / \overline{\tau}_{t}\right), \quad i \in \{1,2\}
  \]</p>
  <p>where \(\overline{\tau}_{s}\) and \(\overline{\tau}_{t}\) are the corresponding temperature parameters for the \(\overline{L}\)-dimensional output distribution. The global cross-view loss is defined as:</p>
  <p>\[
  \mathcal{L}_{cv}^{g} = H(\overline{p}_{1,t}, \overline{p}_{2,s}) +  H(\overline{p}_{2,t}, \overline{p}_{1,s})
  \]</p>
  <p>where \(H(\mathbf{a}, \mathbf{b}) = -\sum_{l=1}^{\overline{L}} \mathbf{a}_{l} \log (\mathbf{b}_{l})\).</p>



  <p>Finally, we obtain the overall training objective of CrIBo as a sum of its individual components:</p>
  <p>\[
  \mathcal{L}_{\text{tot}} = \mathcal{L}_{cv}^{g} + \mathcal{L}_{cv}^{o} + \mathcal{L}_{ci}^{o}
  \]</p>
  <p>The student's parameters are optimized to minimize the above loss whereas the parameters of the teacher are updated via an exponential moving average of the student's weights.</p>




          </div>
        </div>
    </div>
  </section>


  <section class="section" style="background-color:#efeff081">
    <div class="column is-full-width">
      <div class="container is-max-desktop">
      <div class="container is-max-desktop">
        <h2 class="title is-2">Experimental Results</h2>

        <h2 class="title is-4"> 1. CrIBo shows state-of-the-art performance on dense nearest neighbor retrieval task</h2>

        <br> </br>
        <img id="taskaffinity" src="images/Cribo_retrieval.png">
        <figcaption style="text-align: center;">
          <strong>Figure:</strong> <strong>Dense nearest neighbor retrieval</strong>. The quality of the learned spatial features is probed with a k-NN classifier and using different ratios of training data. The depicted mIoU scores are derived from the validation sets of two scene-centric datasets. \( \dagger \) refers to our own reproduction from official GitHub repositories. If not specified, publicly available checkpoints are used. \( \star \) denotes results taken from <a href="https://arxiv.org/pdf/2306.01667.pdf">[Balavzevic et al. (2023)]</a>.


        </figcaption>
        <br> </br>

        <h2 class="title is-4"> 2. CrIBo exhibits commendable performance in linear segmentation</h2>
        <br> </br>
        <img id="taskaffinity" src="images/cribo_linear.png">
        <figcaption style="text-align: center;">
          <strong>Figure:</strong> <strong>Linear segmentation with frozen backbones.</strong> The linear decoder from <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Strudel_Segmenter_Transformer_for_Semantic_Segmentation_ICCV_2021_paper.pdf">[Segmenter]</a> is trained on the frozen spatial features obtained with various self-supervised learning methods. We report the mIoU scores achieved on the validation sets of 4 different datasets. \( \dagger \) refers to our own reproduction from official GitHub repositories. If not specified, publicly available checkpoints are used.


        </figcaption>
        <br> </br>

        <h2 class="title is-4"> 3. CrIBo adeptly evolve into remarkable specialists </h2>

        <br> </br>
        <img id="taskaffinity" src="images/cribo_finetuning.png">
        <figcaption style="text-align: center;">
          <strong>Figure:</strong> <strong>Finetuning evaluation with Segmenter.</strong> Backbones pre-trained with different self-supervised learning methods are finetuned using <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Strudel_Segmenter_Transformer_for_Semantic_Segmentation_ICCV_2021_paper.pdf">[Segmenter]</a>. We report the mIoU scores achieved on the validation sets of 4 different datasets. \( \dagger \) refers to our own reproduction from official GitHub repositories. If not specified, publicly available checkpoints are used.

        </figcaption>
        <br> </br>




      </div>
    </div>
  </section>







<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@inproceedings{Lebailly2024CrIBo,
  title={CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping},
  author={Lebailly, Tim and Stegm{\"u}ller, Thomas and Bozorgtabar, Behzad and Thiran, Jean-Philippe and Tuytelaars, Tinne},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
    }
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://llava-vl.github.io/">LLaVA</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
</section>

</body>

</html>
