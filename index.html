<!DOCTYPE html>
<html>
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-176302866-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-176302866-1');
        </script>

        <meta charset="utf-8">
        <title>Behzad Bozorgtabar</title>
        <meta name="description" content="Project Page">
        <link rel="stylesheet" href="main.css">
        <link rel="stylesheet" href="https://use.typekit.net/oml3bsz.css">
    </head>
    <body>
        <h1>
            Behzad Bozorgtabar
        </h1><br>
        <h3> Computer Vision Group Leader & Lecturer at EPFL<br>
          Senior Scientist at CHUV-EPFL </h3><br>
        <div class="row">
            <div class="column-left">
                <img class="me" src="images/BBB.png" alt="Behzad Bozorgtabar"><br>
                email: behzad.bozorgtabar [at] epfl [dot] ch <br>
                <p> <a href="https://scholar.google.com/citations?user=kxAk6AoAAAAJ"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
        					<a href="https://github.com/BehzadBozorgtabar"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
                  <a href="https://dblp.org/pid/59/10419.html"><img src="./pic/dblp.png" height="30px" style="margin-bottom:-3px"></a>
                  <a href="https://www.linkedin.com/in/behzad-bozorgtabar-72838560/"><img src="./pic/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
        				</p>
            </div>
            <!--<div class="column-right pxb">
            <a href="https://github.com/BehzadBozorgtabar/SELF-TAUGHT-SEMI-SUPERVISED-ANOMALY-DETECTION">github</a></span></p>
                <h1>THE PIXEL BENDING BLOG</h1>
                <p>Welcome to the Pixel Bending mini-blog! Here, I'll talk about various topics related to synthesizing images,
                    building deep generative models and more. Click the links below to access the posts:</p>
                <a class="disentangle-button gradient-button" href="blog_posts/disentanglement.html">1. THE MAGIC OF DISENTANGLEMENT</>
                <a class="hessian-button gradient-button">2. THE HESSIAN PENALTY</a>
            </div>-->

            <div class="column-right pxb">
                <!--<h1>HAPPY NOWRUZ</h1>
                <img  class="middle" src="images/nowruz-2017.gif">-->
                <h1>RESEARCH UPDATES</h1>
                <p><img style="width:60px; height:60px;" class="middle" src="images/Crocodile.gif"> <b>CVPR 2023 paper: 'CrOC: Cross-View Online Clustering for Dense Visual Representation Learning'</b></p><br>
                <a class="disentangle-button gradient-button" href="CrOC.html">CrOC</a>
                <p><img style="width:60px; height:60px;" class="middle" src="images/tesla.gif"> <b>CVPR 2023 paper: 'TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation'</b></p><br>
                <a class="disentangle-button gradient-button" href="TeSLA.html">TeSLA</a>

            </div>

        </div>
        <br>
        <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #ff8888 65%);">About Me</h2><br>

        I'm a senior scientist at the Signal Processing Lab (LTS5) at the Swiss Federal Institute of Technology (EPFL), with a joint affiliation with the Lausanne University Hospital (CHUV) Department of Radiology, Lausanne, Switzerland. At the EPFL-LTS5, I am the computer vision leader for the medical imaging group. I am a member of <a href="https://ellis.eu/members"> European Lab for Learning & Intelligent Systems (ELLIS)</a> and <a href="https://www.epfl.ch/research/domains/epfl-ellis/ellis-society-members/"> EPFL’s ELLIS Unit</a>. I am also a research staff scientist at the Center for Biomedical Imaging (CIBM). Earlier, I was a Postdoctoral Researcher at IBM Research-Australia.


          <p> My principal research area lies at the intersection of computer vision and medical image analysis using machine learning techniques. I have a strong interest in <b>domain adaptation/generalization</b> and <b>self-supervised methodologies</b> on learning from limited data or labels, which I consider major avenues for
            innovation and impact for many vision-based applications. The ultimate goal of my research is to build next-generation intelligent machines that will tackle many of the most challenging problems in responsible AI, including reliability, generalization, and scarcity of data and annotation, enabling AI systems to be safely deployed in real-world high-stakes applications.</p><br><br>
        <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #fffe8c 65%);">News</h2><br>

        <!--<b>[July 2020]</b> I started a blog! It's called Pixel Bending, and I'll be discussing various things related to generative models.
        The first two-part series is on disentanglement.<br>href="https://www.sciepublish.com/index/journals/editors/cvml/21.html">-->
        <b><font color="red">[July 2023]</font></b> Our joint work with KU Leuven, AdaSim: Adaptive Similarity Bootstrapping for Self-Distillation, has received acceptance from ICCV 2023.<br>
        <b><font color="red">[June 2023]</font></b> Two papers have received acceptance from MICCAI 2023.<br>
        <b><font color="red">[Feb 2023]</font></b> Our paper, TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation, has received acceptance from CVPR 2023.<br>
        <b><font color="red">[Feb 2023]</font></b> Our paper, CrOC: Cross-View Online Clustering for Dense Visual Representation Learning, has received acceptance from CVPR 2023.<br>
        <b>[Jan 2023]</b> Our joint work with the University of Bern, namely 'SAGTTA' on test-time augmentation, has been accepted to ISBI 2023.<br>
        <!--<b>[Nov 2022]</b> Our paper on self-supervised anomaly localization has received acceptance from AAAI 2023 (acceptance rate of 19.6%) (Oral).<br>
        <b>[Oct 2022]</b> Our paper won First Runner up award at the AIMIA Workshop at ECCV 2022.<br>
        <b>[Oct 2022]</b> Our new paper, ScoreNet, has received acceptance from WACV 2023.<br>
        <!--<b>[Sep 2022]</b> Our new paper on transformer-based anomaly detection and localization has received acceptance from BMVC 2022.<br>
        <!--<b><b>[Aug 2022]</b> My patent on system and method for domain adaptation has been published.</a><br>
        [<b>[Aug 2022]</b> Two papers have received acceptance from ECCV 2022  AIMIA Workshop</a>.<br>
        <b>[Aug 2022]</b> My patent on annotation-efficient image anomaly detection has been published.</a><br>
        July 2020]</b> I started a blog! It's called Pixel Bending, and I'll be discussing various things related to generative models.
        The first two-part series is on disentanglement.<br>href="https://www.sciepublish.com/index/journals/editors/cvml/21.html">
        <b>[May 2022]</b> I joined the editorial board of a Journal of Computer Vision and Machine Learning</a>.<br>
        <b>[Apr 2022]</b> Code for the OptTTA has been released in PyTorch <a href="https://github.com/devavratTomar/OptTTA"> here</a>.<br>
        <b>[Mar 2022]</b> Our new paper, OptTTA, has received acceptance from MIDL 2022 for an oral presentation.<br>-->


       <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #cd8cff 65%);">EPFL Computer Vision Talks</h2><br>
       <p class="paper"><img src="images/TALKS.png"
                             alt="EPFL CV Talks.">
           <span><b>I am organizing the EPFL Computer Vision Talks</b>
       <br><a href="https://www.youtube.com/channel/UCAkp-rlUGYkpdIqmKezamWw/videos">YouTube Channel</a> </span></p>

       <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #8ca9ff 65%);">Citations</h2><br>
       <p class="paper"><img src="images/citation_updated.png"
                             alt="citations.">
           <span><b>A complete list of my publications and patents can be found at </b>
       <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=kxAk6AoAAAAJ&gmla=AJsN-F7ChUXZmlbUjbrqc1qsOC_ETzCYHSofHkiyiwD9wqLtXswlrGvYRW3LraC0IEF11euu9IZagFBrJWlAoI1OLnTW5ZUMHgLlhwyVYuys03rvbNlVlmzBpuKNVK9RSNOatjiKnp8r">Google Scholar</a> </span></p>



       <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #93fc93 65%);">Recent Selected Publications</h2><br>

       <p class="paper"><img src="images/Adasim.png"
                       alt="Adasim">
       <span><b><font color="red">*New*</font> Adaptive Similarity Bootstrapping for Self-Distillation</b>
       <br>Tim Lebailly, Thomas Stegmüller, <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran, Tinne Tuytelaars</b>
         <br>ICCV 2023
         <br>   <a href="https://arxiv.org/pdf/2303.13606.pdf">paper</a> </span></p>


        <h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #93fc93 65%);">Recent Selected Publications</h2><br>

        <p class="paper"><img src="images/CrOC_main.png"
                        alt="CrOC_main">
        <span><b><font color="red">*New*</font> CrOC: Cross-View Online Clustering for Dense Visual Representation Learning</b>
        <br>Thomas Stegmüller, Tim Lebailly, <b>Behzad Bozorgtabar</b>, Tinne Tuytelaars, Jean-Philippe Thiran</b>
          <br>CVPR 2023
          <br>  <a href="CrOC.html">project page</a> &#183; <a href="https://arxiv.org/pdf/2303.13245.pdf">paper</a> &#183; <a href="https://github.com/stegmuel/CrOC">github</a></span></p>


        <p class="paper"><img src="images/TeSLA_main.png"
                        alt="TeSLA_main">
        <span><b><font color="red">*New*</font> TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation</b>
        <br>Devavrat Tomar, Guillaume Vray, <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
          <br>CVPR 2023
          <br>   <a href="TeSLA.html">project page</a> &#183; <a href="https://arxiv.org/pdf/2303.09870.pdf">paper</a> &#183; <a href="https://github.com/devavratTomar/TeSLA">github</a></span></p>


          <p class="paper"><img src="images/proto.png"
                          alt="Proto SF-OSDA">
          <span><b><font color="red">*New*</font> Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling
Self-Supervised Vision Transformer</b>
          <br>Guillaume Vray, Devavrat Tomar, <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
          <br>arXiv 2023
          <br>   <a href="https://arxiv.org/pdf/2307.04596.pdf">paper</a> </span></p>

          <p class="paper"><img src="images/amae.png"
                          alt="AMAE">
          <span><b><font color="red">*New*</font> AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</b>
          <br><b>Behzad Bozorgtabar</b>, Dwarikanath Mahapatra, Jean-Philippe Thiran</b>
            <br>MICCAI 2023</span></p>

        <p class="paper"><img src="images/scorenet_2.png"
                        alt="ScoreNet">
        <span><b>ScoreNet: Learning Non-Uniform Attention and Augmentation for <br>Transformer-Based Histopathological Image Classification</b>
        <br>Thomas Stegmüller, <b>Behzad Bozorgtabar</b>, Antoine Spahr, Jean-Philippe Thiran</b>
          <br>WACV 2023
          <br>  <a href="ScoreNet.html">project page</a> &#183; <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Stegmuller_ScoreNet_Learning_Non-Uniform_Attention_and_Augmentation_for_Transformer-Based_Histopathological_Image_WACV_2023_paper.pdf">paper</a> &#183; <a href="https://github.com/stegmuel/ScoreNet">github</a> </span></p>

          <p class="paper"><img src="images/BMVC_2022.png"
                          alt="ANOM">
          <span><b>Anomaly Detection and Localization Using Attention-Guided Synthetic Anomaly <br>and Test-Time Adaptation</b>
          <br><b>Behzad Bozorgtabar</b>, Dwarikanath Mahapatra, Jean-Philippe Thiran</b>
            <br>BMVC 2022
            <br>  <a href="https://bmvc2022.mpi-inf.mpg.de/0472.pdf">paper</a>  </span></p>

              <p class="paper"><img src="images/optTTA.png"
                              alt="OptTTA">
            <span><b> OptTTA: Learnable Test-Time Augmentation for <br>Source-Free Medical Image Segmentation Under Domain Shift</b>
        <br>Devavrat Tomar, Guillaume Vray, Jean-Philippe Thiran, <b>Behzad Bozorgtabar</b>
                <br>MIDL 2022 (Oral)
                <br>   <a href="OptTTA.html">project page</a> &#183; <a href="https://openreview.net/pdf?id=B6HdQaY_iR">paper</a> &#183; <a href="https://github.com/devavratTomar/OptTTA">github</a></span></p>



                  <p class="paper"><img src="images/SRMA.png"
                                        alt="SRMA.">
                      <span><b> Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning <br>Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection</b>
                  <br>Christian Abbet, Linda Studer, Andreas Fischer, Heather Dawson, Inti Zlobec,<br>
                  <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                  <br>MedIA Journal 2022
                  <br> <a href="https://www.sciencedirect.com/science/article/pii/S1361841522001207?utm_campaign=STMJ_AUTH_SERV_PUBLISHED&utm_medium=email&utm_acid=90864309&SIS_ID=&dgcid=STMJ_AUTH_SERV_PUBLISHED&CMX_ID=&utm_in=DM256431&utm_source=AC_">paper</a> &#183;
                          <a href="https://github.com/christianabbet/SRA">github</a></span></p>

                      <p class="paper"><img src="images/wacv_2022.png"
                                                alt="SST.">
                              <span><b>Self-Supervised Generative Style Transfer for One-Shot Medical Image Segmentation</b>
                          <br>Devavrat Tomar, <b>Behzad Bozorgtabar</b>, Manana Lortkipanidze, Guillaume Vray, <br>Mohammad Saeed Rad, Jean-Philippe Thiran
                          <br>WACV 2022
                          <br> <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Tomar_Self-Supervised_Generative_Style_Transfer_for_One-Shot_Medical_Image_Segmentation_WACV_2022_paper.pdf">paper</a> &#183;
                                  <a href="https://github.com/devavratTomar/SST/">github</a></span></p>

                                  <p class="paper"><img src="images/SegGini.png"
                                                            alt="SegGini.">
                                          <span><b>Learning Whole-Slide Segmentation from Inexact and <br> Incomplete Labels using Tissue Graphs</b>
                                      <br>Valentin Anklin, Pushpak Pati, Guillaume Jaume, <b>Behzad Bozorgtabar</b>, <br> Antonio Foncubierta-Rodríguez, Jean-Philippe Thiran, Mathilde Sibony,<br> Maria Gabrani, Orcun Goksel
                                      <br>MICCAI 2021
                                      <br> <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-87196-3_59.pdf">paper</a> &#183;
                                              <a href="https://github.com/histocartography/histocartography">github</a></span></p>

                                    <p class="paper"><img src="images/SOoD.png"
                                                              alt="SOoD.">
                                                      <span><b>SOoD: Self-Supervised Out-of-Distribution Detection <br>Under Domain Shift for Multi-Class Colorectal Cancer Tissue Types</b>
                                                  <br><b>Behzad Bozorgtabar</b>, Guillaume Vray, Dwarikanath Mahapatra, Jean-Philippe Thiran
                                                  <br>ICCVW 2021
                                                  <br> <a href="https://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Bozorgtabar_SOoD_Self-Supervised_Out-of-Distribution_Detection_Under_Domain_Shift_for_Multi-Class_Colorectal_ICCVW_2021_paper.pdf">paper</a> &#183;
                                                          <a href="https://github.com/GuillaumeVray/SOoD">github</a></span></p>

                                                          <p class="paper"><img src="images/arxiv.png"
                                                                                    alt="CVPR2021.">
                                                                            <span><b>Quantifying Explainers of Graph Neural Networks in Computational Pathology</b>
                                                                        <br>Guillaume Jaume, Pushpak Pati, <b>Behzad Bozorgtabar</b>, Antonio Foncubierta-Rodríguez, <br>Florinda Feroce, Anna Maria Anniciello, Tilman Rau, Maria Gabrani, <br>Jean-Philippe Thiran, Orcun Goksel
                                                                        <br>CVPR 2021
                                                                        <br> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jaume_Quantifying_Explainers_of_Graph_Neural_Networks_in_Computational_Pathology_CVPR_2021_paper.pdf">paper</a> &#183;
                                                                                <a href="https://github.com/histocartography/patho-quant-explainer">github</a></span></p>


                                                                                <p class="paper"><img src="images/tmi.png"
                                                                                                          alt="T-MI.">
                                                                                                  <span><b>Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation</b>
                                                                                              <br>Devavrat Tomar, Manana Lortkipanidze, Guillaume Vray, <br> <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                                                                                              <br>IEEE T-MI 2021
                                                                                              <br> <a href="https://ieeexplore.ieee.org/document/9354186">paper</a> &#183;
                                                                                                      <a href="https://github.com/devavratTomar/sasan">github</a></span></p>

                                                                                                      <p class="paper"><img src="images/midl.png"
                                                                                                                                alt="MIDL2021.">
                                                                                                                        <span><b>Self-Rule to Adapt: Learning Generalized Features from Sparsely-Labeled Data<br> Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Phenotyping</b>
                                                                                                                    <br>Christian Abbet, Linda Studer, Andreas Fischer, Heather Dawson, Inti Zlobec,<br>
                                                                                                                    <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                                                                                                                    <br>MIDL 2021
                                                                                                                    <br> <a href="https://2021.midl.io/proceedings/abbet21.pdf">paper</a> &#183;
                                                                                                                            <a href="https://github.com/christianabbet/SRA">github</a></span></p>

                                                                                                                            <p class="paper"><img src="images/isbi2021.png"
                                                                                                                                                      alt="ISBI2021.">
                                                                                                                                              <span><b>Self-Taught Semi-Supervised Anomaly Detection on Upper Limb X-rays</b>
                                                                                                                                          <br>Antoine Spahr , <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                                                                                                                                          <br>ISBI 2021
                                                                                                                                          <br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9433771">paper</a>&#183;
                                                                                                                                          <a href="https://github.com/BehzadBozorgtabar/SELF-TAUGHT-SEMI-SUPERVISED-ANOMALY-DETECTION">github</a></span></p>


                                                                                                                                                  <p class="paper"><img src="images/wacv20.png"
                                                                                                                                                                            alt="ISBI2021.">
                                                                                                                                                                    <span><b>Benefiting from Bicubically Down-Sampled Images for <br>Learning Real-World Image Super-Resolution</b>
                                                                                                                                                                <br>Mohammad Saeed Rad, Thomas Yu, Claudiu Musat, Hazım Kemal Ekenel, <br> <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                                                                                                                                                                <br>WACV 2021
                                                                                                                                                                <br> <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Rad_Benefiting_From_Bicubically_Down-Sampled_Images_for_Learning_Real-World_Image_Super-Resolution_WACV_2021_paper.pdf">paper</a>
                                                                                                                                                                    </span></p>

                                                                                                                                                                    <p class="paper"><img src="images/miccai20_1.png"
                                                                                                                                                                                              alt="SALAD.">
                                                                                                                                                                                      <span><b>SALAD: Self-Supervised Aggregation Learning for Anomaly Detection on X-Rays</b>
                                                                                                                                                                                  <br><b>Behzad Bozorgtabar</b>, Dwarikanath Mahapatra, Guillaume Vray, Jean-Philippe Thiran
                                                                                                                                                                                  <br>MICCAI 2020
                                                                                                                                                                                  <br> <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-59710-8_46.pdf">paper</a> &#183;
                                                                                                                                                                                          <a href="https://github.com/GuillaumeVray/SALAD">github</a></span></p>

                                                                                                                                                                                          <p class="paper"><img src="images/miccai20_2.png"
                                                                                                                                                                                                                    alt="Self-Rule.">
                                                                                                                                                                                                            <span><b>Divide-and-Rule: Self- Supervised Learning for Survival Analysis in Colorectal Cancer</b>
                                                                                                                                                                                                        <br>Christian Abbet, Inti Zlobec, <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran
                                                                                                                                                                                                        <br>MICCAI 2020
                                                                                                                                                                                                        <br> <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-59722-1_46.pdf">paper</a> &#183;
                                                                                                                                                                                                                <a href="https://github.com/christianabbet/DnR">github</a></span></p>


                                                                                                                                                                                                                <p class="paper"><img src="images/cvpr2020.png"
                                                                                                                                                                                                                                          alt="CVPR2020.">
                                                                                                                                                                                                                                  <span><b>Pathological Retinal Region Segmentation From OCT Images <br>Using Geometric Relation Based Augmentation</b>
                                                                                                                                                                                                                              <br>Dwarikanath Mahapatra, <b>Behzad Bozorgtabar</b>, Jean-Philippe Thiran, Ling Shao
                                                                                                                                                                                                                              <br>CVPR 2020
                                                                                                                                                                                                                              <br> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Mahapatra_Pathological_Retinal_Region_Segmentation_From_OCT_Images_Using_Geometric_Relation_CVPR_2020_paper.pdf">paper</a>


                                                                                                                                                                                                                              <p class="paper"><img src="images/syndemo.png"
                                                                                                                                                                                                                                                        alt="Syndemo.">
                                                                                                                                                                                                                                                <span><b>SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion</b>
                                                                                                                                                                                                                                            <br><b>Behzad Bozorgtabar</b>, Mohammad Saeed Rad, Dwarikanath Mahapatra, Jean-Philippe Thiran
                                                                                                                                                                                                                                            <br>ICCV 2019
                                                                                                                                                                                                                                            <br> <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Bozorgtabar_SynDeMo_Synergistic_Deep_Feature_Alignment_for_Joint_Learning_of_Depth_ICCV_2019_paper.pdf">paper</a> &#183;
                                                                                                                                                                                                                                                    <a href="http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Bozorgtabar_SynDeMo_Synergistic_Deep_ICCV_2019_supplemental.pdf">supplementary material</a></span></p>


                                                                                                                                                                                                                                                    <p class="paper"><img src="images/srobb.png"
                                                                                                                                                                                                                                                                              alt="SROBB.">
                                                                                                                                                                                                                                                                      <span><b>SROBB: Targeted Perceptual Loss for Single Image Super-Resolution</b>
                                                                                                                                                                                                                                                                  <br>Mohammad Saeed Rad, <b>Behzad Bozorgtabar</b>, Urs-Viktor Marti, <br> Max Basler, Hazım Kemal Ekenel, <br>Jean-Philippe Thiran
                                                                                                                                                                                                                                                                  <br>ICCV 2019
                                                                                                                                                                                                                                                                  <br> <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rad_SROBB_Targeted_Perceptual_Loss_for_Single_Image_Super-Resolution_ICCV_2019_paper.pdf">paper</a> &#183;
                                                                                                                                                                                                                                                                          <a href="http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Rad_SROBB_Targeted_Perceptual_ICCV_2019_supplemental.pdf">supplementary material</a></span></p>


                                                                                                                                                                                                                                                                          <p class="paper"><img src="images/fg19.png"
                                                                                                                                                                                                                                                                                                    alt="FG2019.">
                                                                                                                                                                                                                                                                                            <span><b>Using Photorealistic Face Synthesis and Domain Adaptation <br>to Improve Facial Expression Analysis</b>
                                                                                                                                                                                                                                                                                        <br><b>Behzad Bozorgtabar</b>, Mohammad Saeed Rad, Hazım Kemal Ekenel, Jean-Philippe Thiran
                                                                                                                                                                                                                                                                                        <br>FG 2019
                                                                                                                                                                                                                                                                                        <br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756632&tag=1">paper</a> &#183;
                                                                                                                                                                                                                                                                                                <a href="https://github.com/BehzadBozorgtabar/FSDA">github</a></span></p>


                                                                                                                                                                                                                                                                                                <p class="paper"><img src="images/l2s19.jpg"
                                                                                                                                                                                                                                                                                                                          alt="CVIU2019.">
                                                                                                                                                                                                                                                                                                                  <span><b>Learn to Synthesize and Synthesize to Learn</b>
                                                                                                                                                                                                                                                                                                              <br><b>Behzad Bozorgtabar</b>, Mohammad Saeed Rad, Hazım Kemal Ekenel, Jean-Philippe Thiran
                                                                                                                                                                                                                                                                                                              <br>CVIU 2019
                                                                                                                                                                                                                                                                                                              <br> <a href="https://www.sciencedirect.com/science/article/pii/S1077314219300657?via%3Dihub">paper</a> &#183;
                                                                                                                                                                                                                                                                                                                      <a href="https://github.com/BehzadBozorgtabar/Learn-to-Synthesize-and-Synthesize-to-Learn">github</a></span></p>



                                                                                                                                                                                                                                                                                                                      <p class="paper"><img src="images/NeurIPS18.png"
                                                                                                                                                                                                                                                                                                                                                alt="NeurIPS18.">
                                                                                                                                                                                                                                                                                                                                        <span><b>Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks</b>
                                                                                                                                                                                                                                                                                                                                    <br>Guillaume Jaume, <b>Behzad Bozorgtabar</b>, Hazım Kemal Ekenel, <br>Jean-Philippe Thiran, Maria Gabrani
                                                                                                                                                                                                                                                                                                                                    <br>NeurIPS 2018
                                                                                                                                                                                                                                                                                                                                    <br> <a href="https://arxiv.org/pdf/1811.03830.pdf">paper</a>


                                                                                                                                                                                                                                                                                                                                    <p class="paper"><img src="images/msmct.png"
                                                                                                                                                                                                                                                                                                                                                              alt="MSMCT.">
                                                                                                                                                                                                                                                                                                                                                      <span><b>MSMCT: Multi-State Multi-Camera Tracker</b>
                                                                                                                                                                                                                                                                                                                                                  <br><b>Behzad Bozorgtabar</b>, Roland Goecke
                                                                                                                                                                                                                                                                                                                                                  <br>IEEE TCSVT 2018
                                                                                                                                                                                                                                                                                                                                                  <br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8048028">paper</a>



<!-- <tr><center>
        <p class="paper"><video loop autoplay muted>
            <source src="images/hessian.mp4" type="video/mp4">
        </video>
            <span><b>The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement</b>
        <br>William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio Torralba
        <br>ECCV 2020 (Spotlight)
                <br> <a href="https://youtu.be/uZyIcTkSSXA">video</a> &#183; <a href="https://youtu.be/jPl-0EN6S1w">overview in 90 seconds</a> &#183; <a href="hessian-penalty.html">project page</a> &#183; <a href="https://arxiv.org/abs/2008.10599">paper</a> &#183; <a href="https://github.com/wpeebles/hessian_penalty">github</a></span></p>
                <!-- </center></tr> -->



<!-- <tr><center>
        <p class="paper"><img src="images/sig2019.png"
                              alt="A natural image of a building is modified by adding trees and domes.">
            <span><b>Semantic Photo Manipulation with a Generative Image Prior</b>
        <br>David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba
        <br>SIGGRAPH 2019<br><a href="http://ganpaint.io/demo/?project=church">demo</a> &#183;
        <a href="http://ganpaint.io/Bau_et_al_Semantic_Photo_Manipulation_preprint.pdf">paper</a> &#183;
        <a href="http://ganpaint.io/">overview</a></span></p>
<!-- </center></tr> -->
<h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #ffcd8c 65%);">Grants</h2><br>

<table style="border-spacing:2px">

    <tbody>
      <tr><td> Personalized Health and Related Technologies (PHRT)</td></tr>
    <tr><td> Swiss Cancer League</td></tr>
    <tr><td> Discovery Translation Fund (DTF 2.0)</td></tr>
  </tbody>
</table>

<h2 style="background: linear-gradient(180deg, rgba(255,255,255,0) 65%, #8cfffb 65%);">Teaching</h2><br>

<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2019-Present</td><td>Image analysis and pattern recognition (EE-451-4 ECTS- Bozorgtabar &
Thiran), EPFL </td>
		</tr>
		<tr>
			<td> 2019-Present</td><td>Lab in signal and image processing (EE-490(f)-4 ECTS- Bozorgtabar & Thiran), EPFL</td>
		</tr>

	</tbody>
</table>

    </body>

    <div id="footer">
    	<div id="footer-text"></div>
    </div>
    	<p><center>
          	<div id="clustrmaps-widget" style="width:10%">

              <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=s99PAjUbyKsvGkd9kZqSAJUKuVx8PqiTpvJ3gAQj7mA"></script>
          		<noscript><a href='https://clustrmaps.com/site/xfn5'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=pT8r_ZMBBdBPTv7KnlTCiBDylmHyi1qsWdPpY_tIlqY'/></a></noscript>



    <!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=UexA6kPrFZuJeB69B5BZyS063R_EhdDx6FAwAYiub2U&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
          		<noscript><a href='https://clustrmaps.com/site/1aa2l'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=UexA6kPrFZuJeB69B5BZyS063R_EhdDx6FAwAYiub2U&co=2d78ad&ct=ffffff'/></a></noscript> -->
    	</div>
    	<br>
            &copy; Template from William Peebles. Behzad Bozorgtabar | Last updated: July 14 2023

          </center></p>


    </div>
</html>
